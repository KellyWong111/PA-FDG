#!/usr/bin/env python3
"""
AMLP v2 - è‡ªé€‚åº”å…ƒå­¦ä¹ åŸå‹ç½‘ç»œ

åŸºäºv5.1 (v7) + 3ä¸ªæ ¸å¿ƒåˆ›æ–°:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¿ç•™v5.1çš„æ‰€æœ‰ä¼˜åŠ¿:
1. å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ (é«˜è´¨é‡ç‰¹å¾)
2. å¤šåŸå‹ç½‘ç»œ (è¡¨è¾¾èƒ½åŠ›å¼º)
3. è½¯åŸå‹åŒ¹é… (æ›´é²æ£’)
4. åŠ¨æ€é˜ˆå€¼ä¼˜åŒ– (å¹³è¡¡Sens/Spec)

æ–°å¢AMLPåˆ›æ–°:
5. MDLè‡ªé€‚åº”Kå€¼é€‰æ‹© (æ›¿ä»£å›ºå®šK=3)
6. MAMLå…ƒå­¦ä¹ åˆå§‹åŒ– (æ›¿ä»£K-means)
7. å­¦ä¹ çš„Mahalanobisè·ç¦» (æ›¿ä»£æ¬§æ°è·ç¦»)

ç›®æ ‡: AUC > 0.89, Sens > 0.86, Spec > 0.81
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import os
import sys
import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import time
from copy import deepcopy

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering=1)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', buffering=1)

# v5.1çš„æ ¸å¿ƒç»„ä»¶
from models.cp_protonet import CP_ProtoNet, TripletBuilder, PrototypeManager
from utils.soft_prototype_manager import SoftPrototypeManager, SoftPrototypeLoss

# AMLPçš„åˆ›æ–°ç»„ä»¶
from utils.adaptive_prototype_selector import AdaptivePrototypeSelector
from utils.meta_learned_initializer import MetaLearnedPrototypeInitializer
from utils.learned_distance_metric import DiagonalMahalanobisDistance
from utils.hybrid_prototype_initializer import hybrid_prototype_initialization

# Batch Ensembleæ”¯æŒ
try:
    from models.dstg_oml_model_v2_be import DSTG_Model_V2_BE
    HAS_BATCH_ENSEMBLE = True
except ImportError:
    HAS_BATCH_ENSEMBLE = False
    print("âš ï¸ Batch Ensemble æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ¨¡å‹")

# æ•°æ®å¢å¼ºæ”¯æŒ
try:
    from data_augmentation import DataAugmentation, mixup_criterion
    HAS_DATA_AUGMENTATION = True
except ImportError:
    HAS_DATA_AUGMENTATION = False
    print("âš ï¸ æ•°æ®å¢å¼ºæ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä¸ä½¿ç”¨æ•°æ®å¢å¼º")

# é«˜çº§æ¨¡å‹æ”¯æŒ
try:
    from dstg_oml_model_v2_advanced import DSTG_Model_V2_Advanced
    HAS_ADVANCED_MODEL = True
except ImportError:
    HAS_ADVANCED_MODEL = False
    print("âš ï¸ é«˜çº§æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ¨¡å‹")

# æ··åˆå›¾æ”¯æŒ
try:
    from dstg_oml_model_v2_hybrid import DSTGV2Hybrid
    HAS_HYBRID_GRAPH = True
except ImportError:
    HAS_HYBRID_GRAPH = False
    print("âš ï¸ æ··åˆå›¾æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ¨¡å‹")


def set_seed(seed=42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # ç¡®ä¿å¯é‡å¤æ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def batch_get_features(model, X, device, batch_size=512):
    """åˆ†æ‰¹æå–ç‰¹å¾ï¼Œé¿å…æ˜¾å­˜ä¸è¶³"""
    model.eval()
    features_list = []
    with torch.no_grad():
        for start_idx in range(0, len(X), batch_size):
            end_idx = min(start_idx + batch_size, len(X))
            X_batch = torch.FloatTensor(X[start_idx:end_idx])[:, :418].to(device)
            X_batch = X_batch.view(-1, 22, 19)
            features_batch = model.get_features(X_batch)
            features_list.append(features_batch.cpu())
            del X_batch, features_batch
            torch.cuda.empty_cache()
        features = torch.cat(features_list, dim=0).to(device)
    return features


def load_patient_data(patient_id, feature_dir='chbmit_features'):
    """åŠ è½½æ‚£è€…æ•°æ®"""
    feature_types = ['coherences', 'rellogpower', 'asymm_abs', 
                     'autocorrmat', 'arerror', 'rqachannel_t2e3']
    
    X_list = []
    y = None
    
    for feat_name in feature_types:
        pkl_path = os.path.join(feature_dir, feat_name, f'{patient_id}.pkl')
        if not os.path.exists(pkl_path):
            return None, None
        
        with open(pkl_path, 'rb') as f:
            df = pickle.load(f)
        
        feat_cols = [c for c in df.columns if c.startswith(feat_name)]
        X_list.append(df[feat_cols].values)
        
        if y is None:
            if 'y' in df.columns:
                y = df['y'].values
            elif 'label' in df.columns:
                y = df['label'].values
            elif 'Label' in df.columns:
                y = df['Label'].values
    
    X = np.concatenate(X_list, axis=1)
    
    if y is None:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•æ‰¾åˆ°æ ‡ç­¾åˆ—")
        y = np.zeros(len(X))
    
    return X, y


def find_optimal_threshold_v4(y_true, y_prob, strategy='weighted_youden'):
    """
    v4: åŠ æƒYoudenä¼˜åŒ– - æå‡Specificity
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡
        strategy: ä¼˜åŒ–ç­–ç•¥
            - 'weighted_youden': åŠ æƒYouden (0.45*Sens + 0.55*Spec) - æ¨èv4
            - 'youden_constrained': Youden's J + Sensitivityçº¦æŸ (v3)
            - 'balanced': å¹³è¡¡F1, Sens, Spec
            - 'youden': çº¯Youden's J
    
    Returns:
        optimal_threshold, best_score, metrics_dict
    """
    # æ¸è¿›å¼æœç´¢
    # ç¬¬1é˜¶æ®µ: ç²—æœç´¢
    coarse_thresholds = np.arange(0.3, 0.85, 0.05)
    best_score = -1
    best_thresh = 0.5
    best_metrics = {'sensitivity': 0, 'specificity': 0, 'f1': 0}  # åˆå§‹åŒ–é»˜è®¤å€¼
    
    for thresh in coarse_thresholds:
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        # æ ¹æ®ç­–ç•¥è®¡ç®—å¾—åˆ†
        if strategy == 'weighted_youden':
            # v4æ–°å¢: åŠ æƒYoudenï¼Œåå‘Specificity
            # Sensitivityçº¦æŸ >= 0.65 (æ¯”v3çš„0.60æ›´ä¸¥æ ¼)
            if sensitivity < 0.65:
                continue
            score = 0.45 * sensitivity + 0.55 * specificity - 1
            
        elif strategy == 'youden_constrained':
            # v3: Sensitivityçº¦æŸ >= 0.60
            if sensitivity < 0.60:
                continue
            score = sensitivity + specificity - 1  # Youden's J
            
        elif strategy == 'balanced':
            # å¹³è¡¡ä¸‰ä¸ªæŒ‡æ ‡
            score = 0.4 * f1 + 0.3 * sensitivity + 0.3 * specificity
            
        elif strategy == 'youden':
            # çº¯Youden's J
            score = sensitivity + specificity - 1
        
        if score > best_score:
            best_score = score
            best_thresh = thresh
            best_metrics = {
                'sensitivity': sensitivity,
                'specificity': specificity,
                'f1': f1
            }
    
    # ç¬¬2é˜¶æ®µ: ç»†æœç´¢
    fine_thresholds = np.arange(
        max(0.1, best_thresh - 0.1),
        min(0.9, best_thresh + 0.1),
        0.01
    )
    
    for thresh in fine_thresholds:
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        if strategy == 'weighted_youden':
            if sensitivity < 0.65:
                continue
            score = 0.45 * sensitivity + 0.55 * specificity - 1
        elif strategy == 'youden_constrained':
            if sensitivity < 0.60:
                continue
            score = sensitivity + specificity - 1
        elif strategy == 'balanced':
            score = 0.4 * f1 + 0.3 * sensitivity + 0.3 * specificity
        elif strategy == 'youden':
            score = sensitivity + specificity - 1
        
        if score > best_score:
            best_score = score
            best_thresh = thresh
            best_metrics = {
                'sensitivity': sensitivity,
                'specificity': specificity,
                'f1': f1
            }
    
    return best_thresh, best_score, best_metrics


def find_extreme_sensitivity_threshold(y_true, y_prob):
    """
    æè‡´çµæ•åº¦ä¼˜å…ˆé˜ˆå€¼ä¼˜åŒ– (Sensitivity-First)
    
    ç›®æ ‡: Sensitivity â‰¥ 92% (ç¡¬æ€§é—¨æ§›)
    ç­–ç•¥: åœ¨æ»¡è¶³ Sens â‰¥ 92% çš„å‰æä¸‹ï¼Œé€‰æ‹© Spec æœ€é«˜çš„é˜ˆå€¼
    
    å¦‚æœæ²¡æœ‰é˜ˆå€¼èƒ½è¾¾åˆ° 92%ï¼Œåˆ™é€‰æ‹© Sens æœ€é«˜çš„é˜ˆå€¼
    
    Returns:
        optimal_threshold, best_score, metrics_dict
    """
    best_score = -1
    best_thresh = 0.5
    best_metrics = {'sensitivity': 0, 'specificity': 0, 'f1': 0}
    
    TARGET_SENS = 0.92  # ç›®æ ‡çµæ•åº¦
    
    # æœç´¢èŒƒå›´: 0.05-0.8 (æ›´ä½çš„é˜ˆå€¼æé«˜ Sensitivity)
    for thresh in np.arange(0.05, 0.85, 0.01):
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        # ç­–ç•¥ï¼š
        # 1. å¦‚æœ Sens >= 0.92ï¼Œé€‰ Spec æœ€é«˜çš„
        # 2. å¦‚æœæ²¡æœ‰ä»»ä½•é˜ˆå€¼æ»¡è¶³ Sens >= 0.92ï¼Œåˆ™é€‰ Sens æœ€é«˜çš„
        
        if sensitivity >= TARGET_SENS:
            score = specificity + 10.0  # åŠ 10.0ç¡®ä¿æ»¡è¶³æ¡ä»¶çš„å¾—åˆ†æ°¸è¿œé«˜äºä¸æ»¡è¶³çš„
        else:
            score = sensitivity  # å¦‚æœä¸æ»¡è¶³ç›®æ ‡ï¼Œå°±çº¯æ‹¼çµæ•åº¦
            
        if score > best_score:
            best_score = score
            best_thresh = thresh
            best_metrics = {
                'sensitivity': sensitivity,
                'specificity': specificity,
                'f1': f1
            }
    
    return best_thresh, best_score, best_metrics


def find_ultra_sensitivity_threshold(y_true, y_prob):
    """
    è¶…æ¿€è¿› Sensitivity ä¼˜å…ˆé˜ˆå€¼ä¼˜åŒ–
    
    ç›®æ ‡: Sensitivity â‰¥ 88%
    ç­–ç•¥: 0.7*Sens + 0.3*Spec (æåº¦åå‘ Sensitivity)
    
    Returns:
        optimal_threshold, best_score, metrics_dict
    """
    best_score = -1
    best_thresh = 0.5
    best_metrics = {'sensitivity': 0, 'specificity': 0, 'f1': 0}
    
    # æœç´¢èŒƒå›´: 0.1-0.7 (æ›´ä½çš„é˜ˆå€¼æé«˜ Sensitivity)
    for thresh in np.arange(0.1, 0.75, 0.01):
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        # è¶…æ¿€è¿›ç­–ç•¥: 0.7*Sens + 0.3*Spec
        # åªè¦ Sensitivity â‰¥ 0.88 å°±è€ƒè™‘
        if sensitivity >= 0.88:
            score = 0.7 * sensitivity + 0.3 * specificity
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    # å¦‚æœæ‰¾ä¸åˆ° Sens â‰¥ 0.88 çš„é˜ˆå€¼ï¼Œé™ä½åˆ° 0.80
    if best_score < 0:
        for thresh in np.arange(0.1, 0.75, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            if sensitivity >= 0.80:
                score = 0.7 * sensitivity + 0.3 * specificity
                
                if score > best_score:
                    best_score = score
                    best_thresh = thresh
                    best_metrics = {
                        'sensitivity': sensitivity,
                        'specificity': specificity,
                        'f1': f1
                    }
    
    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨çº¯ Sensitivity æœ€å¤§åŒ–
    if best_score < 0:
        for thresh in np.arange(0.1, 0.75, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            score = sensitivity  # çº¯ Sensitivity
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    return best_thresh, best_score, best_metrics


def find_balanced_threshold(y_true, y_prob):
    """
    å¹³è¡¡é˜ˆå€¼ä¼˜åŒ–: 0.6*Sens + 0.4*Spec
    
    ç›®æ ‡: Sens â‰¥ 85%, Spec â‰¥ 80%
    ç­–ç•¥: æ¯”è¶…æ¿€è¿›æ›´å¹³è¡¡ï¼Œä½†ä»åå‘ Sensitivity
    
    Returns:
        optimal_threshold, best_score, metrics_dict
    """
    best_score = -1
    best_thresh = 0.5
    best_metrics = {'sensitivity': 0, 'specificity': 0, 'f1': 0}
    
    # æœç´¢èŒƒå›´: 0.2-0.8
    for thresh in np.arange(0.2, 0.85, 0.01):
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        # å¹³è¡¡ç­–ç•¥: 0.6*Sens + 0.4*Spec
        # Sens çº¦æŸ â‰¥ 0.75
        if sensitivity >= 0.75:
            score = 0.6 * sensitivity + 0.4 * specificity
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    # å¦‚æœæ‰¾ä¸åˆ° Sens â‰¥ 0.75ï¼Œé™ä½åˆ° 0.70
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            if sensitivity >= 0.70:
                score = 0.6 * sensitivity + 0.4 * specificity
                
                if score > best_score:
                    best_score = score
                    best_thresh = thresh
                    best_metrics = {
                        'sensitivity': sensitivity,
                        'specificity': specificity,
                        'f1': f1
                    }
    
    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æ ‡å‡† Youden's J
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            score = sensitivity + specificity  # Youden's J
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    return best_thresh, best_score, best_metrics


def find_balanced_optimal_threshold(y_true, y_prob):
    """
    æœ€ä¼˜å¹³è¡¡é˜ˆå€¼: æœ€å¤§åŒ– min(Sens, Spec)
    
    ç›®æ ‡: Sens å’Œ Spec éƒ½å°½å¯èƒ½é«˜
    ç­–ç•¥: æ‰¾åˆ° Sens å’Œ Spec éƒ½é«˜çš„é˜ˆå€¼ï¼Œä¸æ˜¯æƒè¡¡
    
    Returns:
        optimal_threshold, best_score, metrics_dict
    """
    best_score = -1
    best_thresh = 0.5
    best_metrics = {'sensitivity': 0, 'specificity': 0, 'f1': 0}
    
    # ç¬¬ä¸€è½®: çº¦æŸ Sens â‰¥ 82%, Spec â‰¥ 82%
    for thresh in np.arange(0.2, 0.85, 0.01):
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        # çº¦æŸ: Sens â‰¥ 82%, Spec â‰¥ 82%
        if sensitivity >= 0.82 and specificity >= 0.82:
            # ç›®æ ‡: æœ€å¤§åŒ– min(Sens, Spec)
            score = min(sensitivity, specificity)
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    # ç¬¬äºŒè½®: å¦‚æœæ‰¾ä¸åˆ°ï¼Œé™ä½çº¦æŸåˆ° 78%
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            if sensitivity >= 0.78 and specificity >= 0.78:
                score = min(sensitivity, specificity)
                
                if score > best_score:
                    best_score = score
                    best_thresh = thresh
                    best_metrics = {
                        'sensitivity': sensitivity,
                        'specificity': specificity,
                        'f1': f1
                    }
    
    # ç¬¬ä¸‰è½®: å¦‚æœè¿˜æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æ ‡å‡† Youden's J
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            score = sensitivity + specificity  # Youden's J
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    return best_thresh, best_score, best_metrics


def find_ultimate_threshold(y_true, y_prob):
    """
    ç»ˆæå¹³è¡¡é˜ˆå€¼: 0.52*Sens + 0.48*Spec
    
    ç›®æ ‡: Sens â‰¥ 87%, Spec â‰¥ 86%
    ç­–ç•¥: æœ€å¹³è¡¡çš„æƒé‡ï¼Œé…åˆç±»åˆ«æƒé‡ä½¿ç”¨
    
    Returns:
        optimal_threshold, best_score, metrics_dict
    """
    best_score = -1
    best_thresh = 0.5
    best_metrics = {'sensitivity': 0, 'specificity': 0, 'f1': 0}
    
    # æœç´¢èŒƒå›´: 0.2-0.8
    for thresh in np.arange(0.2, 0.85, 0.01):
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        # ç»ˆæç­–ç•¥: 0.52*Sens + 0.48*Spec (æœ€å¹³è¡¡)
        # Sens çº¦æŸ â‰¥ 0.75
        if sensitivity >= 0.75:
            score = 0.52 * sensitivity + 0.48 * specificity
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    # å¦‚æœæ‰¾ä¸åˆ° Sens â‰¥ 0.75ï¼Œé™ä½åˆ° 0.70
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            if sensitivity >= 0.70:
                score = 0.52 * sensitivity + 0.48 * specificity
                
                if score > best_score:
                    best_score = score
                    best_thresh = thresh
                    best_metrics = {
                        'sensitivity': sensitivity,
                        'specificity': specificity,
                        'f1': f1
                    }
    
    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æ ‡å‡† Youden's J
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            score = sensitivity + specificity  # Youden's J
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    return best_thresh, best_score, best_metrics


def find_target_threshold(y_true, y_prob):
    """
    ç›®æ ‡ä¼˜åŒ–é˜ˆå€¼: 0.55*Sens + 0.45*Spec
    
    ç›®æ ‡: Sens â‰¥ 87%, Spec â‰¥ 86%
    ç­–ç•¥: ç•¥å¾®åå‘ Sensitivityï¼Œä½†æ›´å¹³è¡¡
    
    Returns:
        optimal_threshold, best_score, metrics_dict
    """
    best_score = -1
    best_thresh = 0.5
    best_metrics = {'sensitivity': 0, 'specificity': 0, 'f1': 0}
    
    # æœç´¢èŒƒå›´: 0.2-0.8
    for thresh in np.arange(0.2, 0.85, 0.01):
        preds = (y_prob > thresh).astype(int)
        
        if len(np.unique(preds)) < 2:
            continue
        
        tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(y_true, preds, zero_division=0)
        
        # ç›®æ ‡ç­–ç•¥: 0.55*Sens + 0.45*Spec
        # Sens çº¦æŸ â‰¥ 0.75
        if sensitivity >= 0.75:
            score = 0.55 * sensitivity + 0.45 * specificity
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    # å¦‚æœæ‰¾ä¸åˆ° Sens â‰¥ 0.75ï¼Œé™ä½åˆ° 0.70
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            if sensitivity >= 0.70:
                score = 0.55 * sensitivity + 0.45 * specificity
                
                if score > best_score:
                    best_score = score
                    best_thresh = thresh
                    best_metrics = {
                        'sensitivity': sensitivity,
                        'specificity': specificity,
                        'f1': f1
                    }
    
    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æ ‡å‡† Youden's J
    if best_score < 0:
        for thresh in np.arange(0.2, 0.85, 0.01):
            preds = (y_prob > thresh).astype(int)
            
            if len(np.unique(preds)) < 2:
                continue
            
            tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = f1_score(y_true, preds, zero_division=0)
            
            score = sensitivity + specificity  # Youden's J
            
            if score > best_score:
                best_score = score
                best_thresh = thresh
                best_metrics = {
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'f1': f1
                }
    
    return best_thresh, best_score, best_metrics


def calculate_metrics_with_threshold(y_true, y_pred, y_prob, threshold):
    """ä½¿ç”¨æŒ‡å®šé˜ˆå€¼è®¡ç®—æŒ‡æ ‡"""
    preds = (y_prob > threshold).astype(int)
    
    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.5
    f1 = f1_score(y_true, preds, zero_division=0)
    tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    
    return {
        'AUC': auc,
        'Accuracy': accuracy,  # æ·»åŠ  Accuracy
        'F1': f1,
        'Sensitivity': sensitivity,
        'Specificity': specificity,
        'Precision': precision,
        'Threshold': threshold
    }


def get_adaptive_sample_size_v3(total_samples, preictal_ratio, patient_id):
    """
    v7 Few-Shoté€‚åº”: è½¯åŸå‹åŒ¹é…
    
    æ”¹è¿›:
    1. æ¯ä¸ªç±»ç”¨Kä¸ªåŸå‹è¡¨ç¤º
    2. K-meansåˆå§‹åŒ–
    3. è½¯åŠ æƒè·ç¦» (è€Œéæœ€å°è·ç¦»)
    4. æ›´é²æ£’ï¼Œå‡å°‘SensæŸå¤±
    """
    # åŸºç¡€æ ·æœ¬æ•°
    if total_samples < 500:
        base_samples = min(150, int(total_samples * 0.35))
    elif total_samples < 1000:
        base_samples = 200
    else:
        base_samples = 300
    
    # å·²çŸ¥å›°éš¾æ‚£è€…å¢åŠ æ ·æœ¬
    difficult_patients = ['chb06', 'chb08', 'chb10', 'chb14', 'chb15']
    if patient_id in difficult_patients:
        base_samples = int(base_samples * 1.3)
        print(f"  âš ï¸  å›°éš¾æ‚£è€…ï¼Œå¢åŠ é€‚åº”æ ·æœ¬åˆ° {base_samples}")
    
    # ç¡®ä¿Preictalæ ·æœ¬è¶³å¤Ÿï¼Œå¹¶ä¸”æµ‹è¯•é›†è‡³å°‘ä¿ç•™20%çš„Preictal
    available_preictal = int(total_samples * preictal_ratio)
    min_test_preictal = max(10, int(available_preictal * 0.20))  # æµ‹è¯•é›†è‡³å°‘ä¿ç•™20%
    max_adapt_preictal = available_preictal - min_test_preictal
    
    # é€‚åº”é›†Preictalç›®æ ‡æ•°é‡ï¼ˆçº¦40%ï¼‰
    target_adapt_preictal = int(base_samples * 0.40)
    
    # å¦‚æœå¯ç”¨çš„Preictalä¸å¤Ÿ
    if max_adapt_preictal < target_adapt_preictal:
        # è°ƒæ•´base_samplesï¼Œç¡®ä¿æµ‹è¯•é›†æœ‰æ ·æœ¬
        base_samples = min(base_samples, int(max_adapt_preictal * 2.5))
        print(f"  âš ï¸  Preictalæ ·æœ¬å°‘({available_preictal}ä¸ª)ï¼Œè°ƒæ•´é€‚åº”æ ·æœ¬åˆ° {base_samples}")
    
    return max(120, base_samples)


def contrastive_pretrain(model, train_patients, device, epochs=50, lr=0.001):
    """é˜¶æ®µ1: å¯¹æ¯”é¢„è®­ç»ƒ"""
    print(f"\n{'='*70}")
    print(f"ğŸ”¥ é˜¶æ®µ1: å¯¹æ¯”é¢„è®­ç»ƒ")
    print(f"{'='*70}")
    
    X_list, y_list, patient_ids = [], [], []
    
    for patient in train_patients:
        X, y = load_patient_data(patient)
        if X is not None:
            print(f"  åŠ è½½ {patient}: {X.shape[0]} æ ·æœ¬, Preictal={np.sum(y==1)}")
            X_list.append(X)
            y_list.append(y)
            patient_ids.append(patient)
    
    scaler = StandardScaler()
    X_all = np.concatenate(X_list, axis=0)
    X_all = scaler.fit_transform(X_all)
    
    X_list_scaled = []
    start_idx = 0
    for X in X_list:
        end_idx = start_idx + len(X)
        X_list_scaled.append(X_all[start_idx:end_idx])
        start_idx = end_idx
    
    triplet_builder = TripletBuilder(margin=1.0)
    triplets = triplet_builder.create_triplets(
        X_list_scaled, y_list, patient_ids, num_triplets=3000
    )
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    print(f"\nå¼€å§‹å¯¹æ¯”é¢„è®­ç»ƒ ({epochs} epochs)...")
    model.train()
    
    best_loss = float('inf')
    patience = 15
    patience_counter = 0
    
    for epoch in range(epochs):
        total_loss = 0
        batch_size = 32
        
        np.random.shuffle(triplets)
        
        for i in range(0, len(triplets), batch_size):
            batch_triplets = triplets[i:i+batch_size]
            
            # æ€§èƒ½ä¼˜åŒ–: å…ˆåˆå¹¶ä¸º numpy æ•°ç»„å†è½¬ tensor (å¿«100å€)
            anchors = torch.FloatTensor(np.array([t['anchor'] for t in batch_triplets]))
            positives = torch.FloatTensor(np.array([t['positive'] for t in batch_triplets]))
            negatives = torch.FloatTensor(np.array([t['negative'] for t in batch_triplets]))
            
            anchors = anchors[:, :418].to(device).view(-1, 22, 19)
            positives = positives[:, :418].to(device).view(-1, 22, 19)
            negatives = negatives[:, :418].to(device).view(-1, 22, 19)
            
            z_anchor = model.get_projected_features(anchors)
            z_positive = model.get_projected_features(positives)
            z_negative = model.get_projected_features(negatives)
            
            loss = triplet_builder.triplet_loss(z_anchor, z_positive, z_negative)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / (len(triplets) / batch_size)
        scheduler.step()
        
        if (epoch + 1) % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"  Epoch {epoch+1:2d} - Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")
        
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  Early stopping at epoch {epoch+1}")
                break
    
    print(f"âœ… å¯¹æ¯”é¢„è®­ç»ƒå®Œæˆï¼æœ€ä½³Loss: {best_loss:.4f}")
    
    return model, scaler


def prototype_few_shot_adapt_v7(model, X_adapt, y_adapt, device, epochs=20, n_prototypes=3, temperature=0.5, maml_initializer=None, use_hybrid=True, class_weight=None):
    """é˜¶æ®µ2: å¤šåŸå‹Few-Shoté€‚åº” (v5 + æ··åˆåˆå§‹åŒ–)"""
    print(f"\n{'='*70}")
    if use_hybrid and maml_initializer is not None:
        print(f"ğŸ“Š é˜¶æ®µ2: Few-Shoté€‚åº” (v5 + æ··åˆåˆå§‹åŒ–ç­–ç•¥)")
    elif maml_initializer is not None:
        print(f"ğŸ“Š é˜¶æ®µ2: Few-Shoté€‚åº” (v5 + MAMLå…ƒå­¦ä¹ )")
    else:
        print(f"ğŸ“Š é˜¶æ®µ2: Few-Shoté€‚åº” (v5 - å¤šåŸå‹)")
    print(f"{'='*70}")
    
    X_adapt_t = torch.FloatTensor(X_adapt)[:, :418].to(device)
    X_adapt_t = X_adapt_t.view(-1, 22, 19)
    y_adapt_t = torch.LongTensor(y_adapt).to(device)
    
    # åˆ›å»ºè½¯åŸå‹ç®¡ç†å™¨
    soft_proto_manager = SoftPrototypeManager(
        feature_dim=128,
        n_prototypes=n_prototypes,
        temperature=temperature
    ).to(device)
    
    # æå–ç‰¹å¾
    model.eval()
    with torch.no_grad():
        features = model.get_features(X_adapt_t)
    
    features_pre = features[y_adapt_t == 1]
    features_int = features[y_adapt_t == 0]
    
    # åŸå‹åˆå§‹åŒ–: æ··åˆç­–ç•¥ (MAML + K-meansè‡ªé€‚åº”) æˆ– å•ä¸€ç­–ç•¥
    if use_hybrid and maml_initializer is not None:
        # ä½¿ç”¨æ··åˆåˆå§‹åŒ–ç­–ç•¥
        soft_proto_manager, selected_strategy, init_metrics = hybrid_prototype_initialization(
            model, X_adapt, y_adapt, maml_initializer, n_prototypes, device, temperature, verbose=True
        )
        print(f"âœ… æ··åˆåˆå§‹åŒ–å®Œæˆ - ç­–ç•¥: {selected_strategy}")
        print(f"   MAML AUC: {init_metrics['auc_maml']:.4f}, K-means AUC: {init_metrics['auc_kmeans']:.4f}")
    elif maml_initializer is not None:
        # ä»…ä½¿ç”¨MAML
        print(f"ğŸ§  ä½¿ç”¨MAMLå…ƒå­¦ä¹ åˆå§‹åŒ–åŸå‹...")
        proto_pre, proto_int = maml_initializer.fast_adapt(
            features, y_adapt_t,
            K_pre=n_prototypes,
            K_int=n_prototypes,
            adapt_steps=10,
            adapt_lr=0.01,
            verbose=False
        )
        soft_proto_manager.proto_preictal.data = proto_pre
        soft_proto_manager.proto_interictal.data = proto_int
        print(f"âœ… MAMLåˆå§‹åŒ–å®Œæˆ (K={n_prototypes}, 10æ­¥å¿«é€Ÿé€‚åº”)")
    else:
        # ä»…ä½¿ç”¨K-means
        soft_proto_manager.initialize_prototypes_kmeans(features_pre, features_int)
        print(f"åˆå§‹åŸå‹è®¡ç®—å®Œæˆ (K-means, K={n_prototypes}, è½¯åŒ¹é…temperature={temperature})")
    
    adapted_model = deepcopy(model)
    
    n_preictal = np.sum(y_adapt == 1)
    n_interictal = np.sum(y_adapt == 0)
    
    # ä½¿ç”¨æŒ‡å®šçš„ç±»åˆ«æƒé‡ï¼Œæˆ–åŠ¨æ€è®¡ç®—
    if class_weight is not None:
        weight = class_weight
        print(f"\nå¼€å§‹Few-Shoté€‚åº” ({epochs} epochs)...")
        print(f"  é€‚åº”æ ·æœ¬: Preictal={n_preictal}, Interictal={n_interictal}")
        print(f"  ç±»åˆ«æƒé‡: {weight:.2f} (å›ºå®š)")
    else:
        weight_ratio = n_interictal / (n_preictal + 1e-8)
        weight = min(weight_ratio * 1.5, 30.0)
        print(f"\nå¼€å§‹Few-Shoté€‚åº” ({epochs} epochs)...")
        print(f"  é€‚åº”æ ·æœ¬: Preictal={n_preictal}, Interictal={n_interictal}")
        print(f"  ç±»åˆ«æƒé‡: {weight:.2f} (åŠ¨æ€)")
    
    # ä¼˜åŒ–å™¨: åŒæ—¶ä¼˜åŒ–ç‰¹å¾æå–å™¨å’ŒåŸå‹
    optimizer = torch.optim.AdamW([
        {'params': adapted_model.parameters(), 'lr': 0.0001},
        {'params': soft_proto_manager.parameters(), 'lr': 0.001}
    ], weight_decay=0.01)
    
    # è½¯åŸå‹æŸå¤±
    soft_loss_fn = SoftPrototypeLoss(lambda_proto=0.1)
    
    criterion = nn.CrossEntropyLoss(
        weight=torch.FloatTensor([1.0, weight]).to(device)
    )
    
    # ä¸¤é˜¶æ®µè®­ç»ƒ - è”åˆä¼˜åŒ–ç‰¹å¾æå–å™¨å’Œè½¯åŸå‹
    # ç¬¬1é˜¶æ®µ: è¾ƒå¤§å­¦ä¹ ç‡
    adapted_model.train()
    soft_proto_manager.train()
    
    for epoch in range(10):
        # æå–ç‰¹å¾
        features = adapted_model.get_features(X_adapt_t)
        
        # ä½¿ç”¨è½¯åŸå‹è®¡ç®—logits
        logits, _, _ = soft_proto_manager(features)
        
        # è½¯åŸå‹æŸå¤± (åˆ†ç±» + åŸå‹è´¨é‡)
        loss, loss_stats = soft_loss_fn(logits, y_adapt_t, soft_proto_manager, features)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(adapted_model.parameters(), max_norm=0.5)
        torch.nn.utils.clip_grad_norm_(soft_proto_manager.parameters(), max_norm=0.5)
        optimizer.step()
        
        # æ¯5ä¸ªepochç”¨K-meansæ›´æ–°åŸå‹
        if (epoch + 1) % 5 == 0:
            with torch.no_grad():
                features = adapted_model.get_features(X_adapt_t)
                features_pre = features[y_adapt_t == 1]
                features_int = features[y_adapt_t == 0]
                soft_proto_manager.initialize_prototypes_kmeans(features_pre, features_int)
            print(f"âœ… K-meansåˆå§‹åŒ–å®Œæˆ")
    
    # ç¬¬2é˜¶æ®µ: æ›´å°å­¦ä¹ ç‡å¾®è°ƒ
    optimizer = torch.optim.AdamW([
        {'params': adapted_model.parameters(), 'lr': 0.0001},
        {'params': soft_proto_manager.parameters(), 'lr': 0.0001}
    ], weight_decay=0.01)
    
    for epoch in range(10):
        # æå–ç‰¹å¾
        features = adapted_model.get_features(X_adapt_t)
        
        # ä½¿ç”¨è½¯åŸå‹è®¡ç®—logits
        logits, _, _ = soft_proto_manager(features)
        
        # è½¯åŸå‹æŸå¤±
        loss, loss_stats = soft_loss_fn(logits, y_adapt_t, soft_proto_manager, features)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(adapted_model.parameters(), max_norm=0.5)
        torch.nn.utils.clip_grad_norm_(soft_proto_manager.parameters(), max_norm=0.5)
        optimizer.step()
    
    print(f"âœ… Few-Shoté€‚åº”å®Œæˆï¼")
    
    # è¿”å›é€‚åº”åçš„æ¨¡å‹å’Œè½¯åŸå‹ç®¡ç†å™¨
    return adapted_model, soft_proto_manager


def cp_protonet_loso_v7(test_patient, train_patients, 
                        num_adapt_samples=None,
                        pretrain_epochs=50,
                        adapt_epochs=20,
                        n_prototypes=3,
                        device='cuda'):
    """CP-ProtoNet LOSO v7 - è½¯åŸå‹åŒ¹é…ç‰ˆ"""
    print(f"\n{'='*70}")
    print(f"ğŸš€ CP-ProtoNet LOSO v7 - æµ‹è¯•æ‚£è€…: {test_patient} (è½¯åŸå‹K={n_prototypes})")
    print(f"{'='*70}")
    
    device = torch.device(device if torch.cuda.is_available() else 'cpu')
    
    X_test, y_test = load_patient_data(test_patient)
    if X_test is None:
        print(f"âŒ æ— æ³•åŠ è½½æµ‹è¯•æ‚£è€…æ•°æ®")
        return None
    
    print(f"\næµ‹è¯•æ‚£è€…æ•°æ®:")
    print(f"  æ€»æ ·æœ¬: {len(X_test)}")
    print(f"  Preictal: {np.sum(y_test==1)} ({np.sum(y_test==1)/len(y_test)*100:.1f}%)")
    print(f"  Interictal: {np.sum(y_test==0)} ({np.sum(y_test==0)/len(y_test)*100:.1f}%)")
    
    # v3: æ™ºèƒ½é€‚åº”æ ·æœ¬æ•°
    if num_adapt_samples is None:
        preictal_ratio = np.sum(y_test==1) / len(y_test)
        num_adapt_samples = get_adaptive_sample_size_v3(
            len(y_test), preictal_ratio, test_patient
        )
    
    print(f"  é€‚åº”æ ·æœ¬æ•°: {num_adapt_samples}")
    
    # åˆ†å‰²æ•°æ®
    preictal_idx = np.where(y_test == 1)[0]
    interictal_idx = np.where(y_test == 0)[0]
    
    np.random.shuffle(preictal_idx)
    np.random.shuffle(interictal_idx)
    
    n_adapt_preictal = min(int(num_adapt_samples * 0.4), len(preictal_idx))
    n_adapt_interictal = num_adapt_samples - n_adapt_preictal
    
    adapt_idx = np.concatenate([
        preictal_idx[:n_adapt_preictal],
        interictal_idx[:n_adapt_interictal]
    ])
    
    test_idx = np.concatenate([
        preictal_idx[n_adapt_preictal:],
        interictal_idx[n_adapt_interictal:]
    ])
    
    X_adapt, y_adapt = X_test[adapt_idx], y_test[adapt_idx]
    X_eval, y_eval = X_test[test_idx], y_test[test_idx]
    
    print(f"\næ•°æ®åˆ†å‰²:")
    print(f"  é€‚åº”é›†: {len(X_adapt)} (Pre: {np.sum(y_adapt==1)}, Inter: {np.sum(y_adapt==0)})")
    print(f"  æµ‹è¯•é›†: {len(X_eval)} (Pre: {np.sum(y_eval==1)}, Inter: {np.sum(y_eval==0)})")
    
    # åˆ›å»ºæ¨¡å‹
    # æ”¯æŒæ··åˆå›¾
    use_hybrid = getattr(cp_protonet_loso_v7, '_use_hybrid_graph', False)
    fusion_mode = getattr(cp_protonet_loso_v7, '_fusion_mode', 'learned')
    fixed_alpha = getattr(cp_protonet_loso_v7, '_fixed_alpha', 0.6)
    
    model = CP_ProtoNet(
        use_hybrid_graph=use_hybrid,
        fusion_mode=fusion_mode,
        fixed_alpha=fixed_alpha
    ).to(device)
    
    # é˜¶æ®µ1: å¯¹æ¯”é¢„è®­ç»ƒ
    model, scaler = contrastive_pretrain(
        model, train_patients, device, 
        epochs=pretrain_epochs, lr=0.001
    )
    
    X_adapt = scaler.transform(X_adapt)
    X_eval = scaler.transform(X_eval)
    
    # ========== é˜¶æ®µ1.5: MAMLå…ƒè®­ç»ƒ (AMLPæ–°å¢) ==========
    # æ£€æŸ¥æ˜¯å¦ç¦ç”¨MAML
    disable_maml = getattr(cp_protonet_loso_v7, '_disable_maml', False)
    
    if disable_maml:
        print(f"\n{'='*70}")
        print(f"âš ï¸  MAMLå·²ç¦ç”¨ - è·³è¿‡å…ƒå­¦ä¹ é˜¶æ®µ")
        print(f"{'='*70}")
        maml_initializer = None
    else:
        print(f"\n{'='*70}")
        print(f"ğŸ§  é˜¶æ®µ1.5: MAMLå…ƒå­¦ä¹  - å­¦ä¹ è·¨æ‚£è€…å…ƒåŸå‹")
        print(f"{'='*70}")
        
        # åˆ›å»ºMAMLåˆå§‹åŒ–å™¨
        maml_initializer = MetaLearnedPrototypeInitializer(
            feature_dim=128,  # CP-ProtoNetçš„ç‰¹å¾ç»´åº¦
            max_K=10,
            device=str(device)  # ç¡®ä¿deviceæ˜¯å­—ç¬¦ä¸²
        ).to(device)  # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        
        # å‡†å¤‡å…ƒè®­ç»ƒæ•°æ®
        print(f"\nå‡†å¤‡å…ƒè®­ç»ƒæ•°æ® (ä»{len(train_patients)}ä¸ªè®­ç»ƒæ‚£è€…)...")
        meta_train_data = []
        
        for i, train_patient in enumerate(train_patients, 1):  # ä½¿ç”¨å…¨éƒ¨17ä¸ªè®­ç»ƒæ‚£è€…
            X_train, y_train = load_patient_data(train_patient)
            if X_train is None:
                continue
            
            # æ ‡å‡†åŒ–
            X_train = scaler.transform(X_train)
            
            # æå–ç‰¹å¾ (åˆ†æ‰¹å¤„ç†ï¼Œé¿å…æ˜¾å­˜ä¸è¶³)
            model.eval()
            features_list = []
            batch_size = 512  # å‡å°æ‰¹æ¬¡å¤§å°
            with torch.no_grad():
                for start_idx in range(0, len(X_train), batch_size):
                    end_idx = min(start_idx + batch_size, len(X_train))
                    X_batch = torch.FloatTensor(X_train[start_idx:end_idx])[:, :418].to(device)
                    X_batch = X_batch.view(-1, 22, 19)
                    features_batch = model.get_features(X_batch)
                    features_list.append(features_batch.cpu())  # ç§»åˆ°CPUé‡Šæ”¾æ˜¾å­˜
                    del X_batch, features_batch
                    torch.cuda.empty_cache()
                features_train = torch.cat(features_list, dim=0).to(device)
            
            # é‡‡æ ·supportå’Œquery (å¹³è¡¡é‡‡æ ·)
            n_samples = len(features_train)
            n_support = min(150, int(n_samples * 0.5))
            
            # å¹³è¡¡é‡‡æ ·
            pre_idx = (y_train == 1).nonzero()[0]
            int_idx = (y_train == 0).nonzero()[0]
            
            n_sup_pre = min(int(n_support * 0.4), len(pre_idx))
            n_sup_int = n_support - n_sup_pre
            
            if len(pre_idx) >= n_sup_pre and len(int_idx) >= n_sup_int:
                sup_pre = np.random.choice(pre_idx, n_sup_pre, replace=False)
                sup_int = np.random.choice(int_idx, n_sup_int, replace=False)
                support_idx = np.concatenate([sup_pre, sup_int])
                
                # å‰©ä½™ä½œä¸ºquery
                query_mask = np.ones(n_samples, dtype=bool)
                query_mask[support_idx] = False
                query_idx = np.where(query_mask)[0]
                
                if len(query_idx) > 0:
                    meta_train_data.append({
                        'support_features': features_train[support_idx],
                        'support_labels': torch.LongTensor(y_train[support_idx]).to(device),
                        'query_features': features_train[query_idx],
                        'query_labels': torch.LongTensor(y_train[query_idx]).to(device)
                    })
                    print(f"  {train_patient}: Support={len(support_idx)}, Query={len(query_idx)}")
        
        print(f"\nâœ… å‡†å¤‡äº†{len(meta_train_data)}ä¸ªå…ƒè®­ç»ƒä»»åŠ¡")
        print(f"\nå¼€å§‹MAMLå…ƒè®­ç»ƒ (æ”¹è¿›ç‰ˆ)...")
        print(f"  æ”¹è¿›1: ä½¿ç”¨å…¨éƒ¨{len(meta_train_data)}ä¸ªè®­ç»ƒæ‚£è€…")
        print(f"  æ”¹è¿›2: å¢åŠ å…ƒè®­ç»ƒè½®æ•°åˆ°100 epochs")
        print(f"  æ”¹è¿›3: å¢åŠ inner stepsåˆ°10æ­¥")
        if len(meta_train_data) > 0:
            maml_initializer.meta_train(
                meta_train_data,
                meta_epochs=100,  # æ”¹è¿›: ä»30å¢åŠ åˆ°100
                K_pre=n_prototypes,
                K_int=n_prototypes,
                meta_lr=0.001,
                inner_lr=0.01,
                inner_steps=10  # æ”¹è¿›: ä»5å¢åŠ åˆ°10
            )
            print(f"âœ… MAMLå…ƒè®­ç»ƒå®Œæˆï¼")
        else:
            print(f"  å…ƒè®­ç»ƒæ•°æ®ä¸è¶³ï¼Œè·³è¿‡MAML")
            maml_initializer = None
    
    # é˜¶æ®µ2: Few-Shoté€‚åº” (è½¯åŸå‹ + K-means only, åƒv5.1)
    # æ”¯æŒè‡ªå®šä¹‰ç±»åˆ«æƒé‡
    custom_class_weight = getattr(cp_protonet_loso_v7, '_class_weight', None)
    adapted_model, soft_proto_manager = prototype_few_shot_adapt_v7(
        model, X_adapt, y_adapt, device, 
        epochs=adapt_epochs,
        n_prototypes=n_prototypes,
        maml_initializer=None,  # ä¸ç”¨MAMLï¼Œçº¯K-means
        use_hybrid=False,  # ä¸ç”¨æ··åˆç­–ç•¥ï¼Œçº¯K-means
        class_weight=custom_class_weight  # è‡ªå®šä¹‰ç±»åˆ«æƒé‡
    )
    
    # é˜¶æ®µ3: è¯„ä¼° + v7è½¯åŸå‹é˜ˆå€¼ä¼˜åŒ–
    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ é˜¶æ®µ3: è¯„ä¼° + v7è½¯åŸå‹é˜ˆå€¼ä¼˜åŒ–")
    print(f"{'='*70}")
    
    adapted_model.eval()
    soft_proto_manager.eval()
    
    # åœ¨é€‚åº”é›†ä¸Šæ‰¾æœ€ä¼˜é˜ˆå€¼
    with torch.no_grad():
        features_adapt = batch_get_features(adapted_model, X_adapt, device, batch_size=512)
        logits_adapt, _, _ = soft_proto_manager(features_adapt)
        probs_adapt = torch.softmax(logits_adapt, dim=1)[:, 1].cpu().numpy()
    
    # v4: ä½¿ç”¨åŠ æƒYouden (åå‘Specificity)
    # å¯ä»¥é€šè¿‡å‚æ•°æ§åˆ¶é˜ˆå€¼ç­–ç•¥
    use_extreme_sens = getattr(cp_protonet_loso_v7, '_use_extreme_sens', False)
    use_ultra_sens = getattr(cp_protonet_loso_v7, '_use_ultra_sens', False)
    use_balanced = getattr(cp_protonet_loso_v7, '_use_balanced_threshold', False)
    use_target = getattr(cp_protonet_loso_v7, '_use_target_threshold', False)
    use_ultimate = getattr(cp_protonet_loso_v7, '_use_ultimate_threshold', False)
    use_balanced_optimal = getattr(cp_protonet_loso_v7, '_use_balanced_optimal', False)
    
    if use_extreme_sens:
        optimal_thresh, best_score, adapt_metrics = find_extreme_sensitivity_threshold(
            y_adapt, probs_adapt
        )
        print(f"\næè‡´çµæ•åº¦ä¼˜å…ˆé˜ˆå€¼æœç´¢ (Sensâ‰¥92%, ç„¶åæœ€å¤§åŒ–Spec):")
    elif use_ultra_sens:
        optimal_thresh, best_score, adapt_metrics = find_ultra_sensitivity_threshold(
            y_adapt, probs_adapt
        )
        print(f"\nè¶…æ¿€è¿› Sensitivity é˜ˆå€¼æœç´¢ (0.7*Sens + 0.3*Spec, Sensâ‰¥0.80):")
    elif use_balanced:
        optimal_thresh, best_score, adapt_metrics = find_balanced_threshold(
            y_adapt, probs_adapt
        )
        print(f"\nå¹³è¡¡é˜ˆå€¼æœç´¢ (0.6*Sens + 0.4*Spec, Sensâ‰¥0.75):")
    elif use_target:
        optimal_thresh, best_score, adapt_metrics = find_target_threshold(
            y_adapt, probs_adapt
        )
        print(f"\nç›®æ ‡é˜ˆå€¼æœç´¢ (0.55*Sens + 0.45*Spec, Sensâ‰¥0.75):")
    elif use_ultimate:
        optimal_thresh, best_score, adapt_metrics = find_ultimate_threshold(
            y_adapt, probs_adapt
        )
        print(f"\nç»ˆæå¹³è¡¡é˜ˆå€¼æœç´¢ (0.52*Sens + 0.48*Spec, Sensâ‰¥0.75):")
    elif use_balanced_optimal:
        optimal_thresh, best_score, adapt_metrics = find_balanced_optimal_threshold(
            y_adapt, probs_adapt
        )
        print(f"\næœ€ä¼˜å¹³è¡¡é˜ˆå€¼æœç´¢ (max min(Sens, Spec), Sensâ‰¥82%, Specâ‰¥82%):")
    else:
        optimal_thresh, best_score, adapt_metrics = find_optimal_threshold_v4(
            y_adapt, probs_adapt, strategy='weighted_youden'
        )
        print(f"\næœ€ä¼˜é˜ˆå€¼æœç´¢ (åŠ æƒYouden: 0.45*Sens + 0.55*Spec, Sensâ‰¥0.65):")
    
    print(f"  æœ€ä¼˜é˜ˆå€¼: {optimal_thresh:.3f}")
    print(f"  åŠ æƒå¾—åˆ†: {best_score:.4f}")
    print(f"  é€‚åº”é›†Sens: {adapt_metrics['sensitivity']:.4f}")
    print(f"  é€‚åº”é›†Spec: {adapt_metrics['specificity']:.4f}")
    
    # å›°éš¾æ‚£è€…æ£€æµ‹ - æ›´ä¸¥æ ¼çš„æ¡ä»¶
    adapt_auc = roc_auc_score(y_adapt, probs_adapt)
    
    # å¦‚æœæ‰¾ä¸åˆ°æ»¡è¶³çº¦æŸçš„é˜ˆå€¼(best_scoreä»ä¸º-1)ï¼Œæˆ–è€…Senså¤ªä½ï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­–ç•¥
    if best_score < 0 or adapt_metrics['sensitivity'] < 0.50 or adapt_auc < 0.55:
        print(f"\nâš ï¸  çº¦æŸé˜ˆå€¼ä¸é€‚ç”¨ï¼Œå°è¯•å®½æ¾ç­–ç•¥ (Youden's Jæ— çº¦æŸ)")
        # ä½¿ç”¨çº¯Youden's Jï¼ˆæ— Sensitivityçº¦æŸï¼‰
        optimal_thresh_relaxed, _, adapt_metrics_relaxed = find_optimal_threshold_v4(
            y_adapt, probs_adapt, strategy='youden'
        )
        
        # å¦‚æœå®½æ¾ç­–ç•¥çš„Sensitivityä»ç„¶å¤ªä½ï¼Œå›é€€åˆ°å›ºå®šé˜ˆå€¼
        if adapt_metrics_relaxed['sensitivity'] < 0.30:
            print(f"  âš ï¸  å®½æ¾ç­–ç•¥ä»ä¸ç†æƒ³ï¼Œå›é€€åˆ°å›ºå®šé˜ˆå€¼0.5")
            optimal_thresh = 0.5
            strategy_used = 'fixed_fallback'
        else:
            print(f"  âœ… ä½¿ç”¨å®½æ¾ç­–ç•¥: Sens {adapt_metrics_relaxed['sensitivity']:.4f}, Spec {adapt_metrics_relaxed['specificity']:.4f}")
            optimal_thresh = optimal_thresh_relaxed
            adapt_metrics = adapt_metrics_relaxed
            strategy_used = 'dynamic_relaxed'
    else:
        strategy_used = 'dynamic'
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    with torch.no_grad():
        features_eval = batch_get_features(adapted_model, X_eval, device, batch_size=512)
        logits_eval, _, _ = soft_proto_manager(features_eval)
        probs_eval = torch.softmax(logits_eval, dim=1)[:, 1].cpu().numpy()
        preds_eval = (probs_eval > optimal_thresh).astype(int)
    
    metrics = calculate_metrics_with_threshold(
        y_eval, preds_eval, probs_eval, optimal_thresh
    )
    metrics['Strategy'] = strategy_used
    
    print(f"\næµ‹è¯•é›†ç»“æœ:")
    print(f"  AUC: {metrics['AUC']:.4f}")
    print(f"  F1: {metrics['F1']:.4f}")
    print(f"  Sensitivity: {metrics['Sensitivity']:.4f}")
    print(f"  Specificity: {metrics['Specificity']:.4f}")
    print(f"  Precision: {metrics['Precision']:.4f}")
    print(f"  ä½¿ç”¨é˜ˆå€¼: {metrics['Threshold']:.3f} ({strategy_used})")
    
    return metrics


def cp_protonet_loso_v8_ultra_sens(test_patient, train_patients, 
                                     pretrain_epochs=150, adapt_epochs=80, 
                                     n_prototypes=10, device='cuda'):
    """
    v8: è¶…æ¿€è¿› Sensitivity ä¼˜å…ˆç‰ˆæœ¬
    
    åŸºäº v7ï¼Œä½†ä½¿ç”¨è¶…æ¿€è¿›é˜ˆå€¼ç­–ç•¥:
    - 0.7*Sens + 0.3*Spec
    - ç›®æ ‡ Sensitivity â‰¥ 85%
    """
    print(f"\n{'='*70}")
    print(f"ğŸš€ CP-ProtoNet v8 - è¶…æ¿€è¿› Sensitivity ä¼˜å…ˆ")
    print(f"   æµ‹è¯•æ‚£è€…: {test_patient}")
    print(f"   é…ç½®: epochs {pretrain_epochs}/{adapt_epochs}, K={n_prototypes}")
    print(f"{'='*70}")
    
    # åŠ è½½æ•°æ®
    print(f"\nåŠ è½½è®­ç»ƒæ•°æ®...")
    X_train_list, y_train_list = [], []
    for train_patient in train_patients:
        X, y = load_patient_data(train_patient)
        if X is not None:
            X_train_list.append(X)
            y_train_list.append(y)
            print(f"  åŠ è½½ {train_patient}: {len(X)} æ ·æœ¬, Preictal={np.sum(y==1)}")
    
    X_train = np.vstack(X_train_list)
    y_train = np.hstack(y_train_list)
    
    print(f"\nåŠ è½½æµ‹è¯•æ•°æ®...")
    X_test, y_test = load_patient_data(test_patient)
    print(f"  åŠ è½½ {test_patient}: {len(X_test)} æ ·æœ¬, Preictal={np.sum(y_test==1)}")
    
    # åˆ’åˆ†é€‚åº”é›†å’Œè¯„ä¼°é›†
    n_adapt = min(100, len(X_test) // 2)
    X_adapt, y_adapt = X_test[:n_adapt], y_test[:n_adapt]
    X_eval, y_eval = X_test[n_adapt:], y_test[n_adapt:]
    
    print(f"\næ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  é€‚åº”é›†: {len(X_adapt)} æ ·æœ¬")
    print(f"  è¯„ä¼°é›†: {len(X_eval)} æ ·æœ¬")
    
    # é˜¶æ®µ1: å¯¹æ¯”é¢„è®­ç»ƒ
    print(f"\n{'='*70}")
    print(f"ğŸ“Š é˜¶æ®µ1: å¯¹æ¯”é¢„è®­ç»ƒ ({pretrain_epochs} epochs)")
    print(f"{'='*70}")
    
    model = CP_ProtoNet(num_channels=22, feature_dim=19, hidden_dim=128).to(device)
    triplet_builder = TripletBuilder()
    
    X_train_t = torch.FloatTensor(X_train)[:, :418].to(device)
    X_train_t = X_train_t.view(-1, 22, 19)
    y_train_t = torch.LongTensor(y_train).to(device)
    
    # æ³¨æ„: TripletBuilder éœ€è¦ patient_ids (å­—ç¬¦ä¸²åˆ—è¡¨)
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨å•ä¸ªè™šæ‹Ÿæ‚£è€…
    triplets = triplet_builder.create_triplets(
        [X_train_t.cpu().numpy()], [y_train_t.cpu().numpy()], ['train'], num_triplets=3000
    )
    print(f"âœ… æˆåŠŸåˆ›å»º {len(triplets)} ä¸ªä¸‰å…ƒç»„")
    
    print(f"å¼€å§‹å¯¹æ¯”é¢„è®­ç»ƒ ({pretrain_epochs} epochs)...")
    model.contrastive_pretrain(triplets, epochs=pretrain_epochs, lr=0.001, device=device)
    print(f"âœ… å¯¹æ¯”é¢„è®­ç»ƒå®Œæˆï¼")
    
    # é˜¶æ®µ2: Few-Shoté€‚åº”
    adapted_model, soft_proto_manager = prototype_few_shot_adapt_v7(
        model, X_adapt, y_adapt, device, 
        epochs=adapt_epochs,
        n_prototypes=n_prototypes,
        temperature=0.5,
        maml_initializer=None,
        use_hybrid=False
    )
    
    adapted_model.eval()
    soft_proto_manager.eval()
    
    # åœ¨é€‚åº”é›†ä¸Šæ‰¾æœ€ä¼˜é˜ˆå€¼ - ä½¿ç”¨è¶…æ¿€è¿› Sensitivity ç­–ç•¥
    with torch.no_grad():
        features_adapt = batch_get_features(adapted_model, X_adapt, device, batch_size=512)
        logits_adapt, _, _ = soft_proto_manager(features_adapt)
        probs_adapt = torch.softmax(logits_adapt, dim=1)[:, 1].cpu().numpy()
    
    # ä½¿ç”¨è¶…æ¿€è¿› Sensitivity ä¼˜å…ˆç­–ç•¥
    optimal_thresh, best_score, adapt_metrics = find_ultra_sensitivity_threshold(
        y_adapt, probs_adapt
    )
    
    print(f"\nè¶…æ¿€è¿› Sensitivity é˜ˆå€¼æœç´¢ (0.7*Sens + 0.3*Spec, Sensâ‰¥0.80):")
    print(f"  æœ€ä¼˜é˜ˆå€¼: {optimal_thresh:.3f}")
    print(f"  åŠ æƒå¾—åˆ†: {best_score:.4f}")
    print(f"  é€‚åº”é›†Sens: {adapt_metrics['sensitivity']:.4f}")
    print(f"  é€‚åº”é›†Spec: {adapt_metrics['specificity']:.4f}")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    with torch.no_grad():
        features_eval = batch_get_features(adapted_model, X_eval, device, batch_size=512)
        logits_eval, _, _ = soft_proto_manager(features_eval)
        probs_eval = torch.softmax(logits_eval, dim=1)[:, 1].cpu().numpy()
        preds_eval = (probs_eval > optimal_thresh).astype(int)
    
    metrics = calculate_metrics_with_threshold(
        y_eval, preds_eval, probs_eval, optimal_thresh
    )
    metrics['Strategy'] = 'ultra_sensitivity'
    
    print(f"\næµ‹è¯•é›†ç»“æœ:")
    print(f"  AUC: {metrics['AUC']:.4f}")
    print(f"  F1: {metrics['F1']:.4f}")
    print(f"  Sensitivity: {metrics['Sensitivity']:.4f}")
    print(f"  Specificity: {metrics['Specificity']:.4f}")
    print(f"  Precision: {metrics['Precision']:.4f}")
    print(f"  ä½¿ç”¨é˜ˆå€¼: {metrics['Threshold']:.3f} (ultra_sensitivity)")
    
    return metrics


def cp_protonet_loso_v11(
    test_patient, train_patients,
    pretrain_epochs=150, adapt_epochs=80, K=10,
    class_weight=5.0, sens_weight=0.53, spec_weight=0.47,
    device='cuda', verbose=True
):
    """
    v11: v4åŸºç¡€ + ç±»åˆ«æƒé‡5.0 + å¾®è°ƒé˜ˆå€¼(0.53:0.47)
    ç›®æ ‡: AUCâ‰¥88%, Sensâ‰¥87%, Specâ‰¥86% (è¶…è¶Šè®ºæ–‡3ä¸ªç‚¹)
    
    æ”¹è¿›:
    1. åŸºäºv4 (epochs 150/80, K=10) - æœ€å¼ºAUCåŸºç¡€
    2. ç±»åˆ«æƒé‡5.0 - å¼ºåŒ–Preictalåˆ¤åˆ«èƒ½åŠ›
    3. é˜ˆå€¼ç­–ç•¥0.53*Sens + 0.47*Spec - ç•¥åå‘Sensä½†å…¼é¡¾Spec
    4. å›°éš¾æ‚£è€…é¢å¤–è®­ç»ƒ - chb05,06,10,14,15 epochs 200/100
    """
    if verbose:
        print(f"\n{'='*80}")
        print(f"v11: v4åŸºç¡€ + ç±»åˆ«æƒé‡{class_weight} + å¾®è°ƒé˜ˆå€¼({sens_weight}:{spec_weight})")
        print(f"æµ‹è¯•æ‚£è€…: {test_patient}")
        print(f"è®­ç»ƒæ‚£è€…: {len(train_patients)} ä¸ª")
        print(f"{'='*80}\n")
    
    # å›°éš¾æ‚£è€…åˆ—è¡¨
    difficult_patients = ['chb05', 'chb06', 'chb10', 'chb14', 'chb15']
    
    # å¦‚æœæ˜¯å›°éš¾æ‚£è€…ï¼Œå¢åŠ è®­ç»ƒepochs
    if test_patient in difficult_patients:
        pretrain_epochs = 200
        adapt_epochs = 100
        if verbose:
            print(f"âš ï¸ {test_patient} æ˜¯å›°éš¾æ‚£è€…ï¼Œå¢åŠ è®­ç»ƒ: epochs {pretrain_epochs}/{adapt_epochs}")
    
    # åŠ è½½æ•°æ®
    data_dir = 'chbmit_features'  # ä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾ç›®å½•
    
    # è®­ç»ƒé›†
    X_train_list, y_train_list = [], []
    for p in train_patients:
        X, y = load_patient_data(p, data_dir)
        if X is None:
            if verbose:
                print(f"âš ï¸ è·³è¿‡æ‚£è€… {p}: æ•°æ®åŠ è½½å¤±è´¥")
            continue
        X_train_list.append(X)
        y_train_list.append(y)
    
    X_train = np.vstack(X_train_list)
    y_train = np.hstack(y_train_list)
    
    # æµ‹è¯•æ‚£è€…æ•°æ®
    X_test, y_test = load_patient_data(test_patient, data_dir)
    if X_test is None:
        raise ValueError(f"æ— æ³•åŠ è½½æµ‹è¯•æ‚£è€… {test_patient} çš„æ•°æ®")
    
    # åˆ’åˆ†é€‚åº”é›†å’Œè¯„ä¼°é›†
    X_adapt, X_eval, y_adapt, y_eval = train_test_split(
        X_test, y_test, test_size=0.5, random_state=42, stratify=y_test
    )
    
    if verbose:
        print(f"\næ•°æ®ç»Ÿè®¡:")
        print(f"  è®­ç»ƒé›†: {X_train.shape}, Preictal: {y_train.sum()}/{len(y_train)}")
        print(f"  é€‚åº”é›†: {X_adapt.shape}, Preictal: {y_adapt.sum()}/{len(y_adapt)}")
        print(f"  è¯„ä¼°é›†: {X_eval.shape}, Preictal: {y_eval.sum()}/{len(y_eval)}")
    
    # é˜¶æ®µ1: é¢„è®­ç»ƒ (ä½¿ç”¨ç±»åˆ«æƒé‡)
    if verbose:
        print(f"\n{'='*80}")
        print(f"é˜¶æ®µ1: é¢„è®­ç»ƒ (epochs={pretrain_epochs}, K={K}, ç±»åˆ«æƒé‡={class_weight})")
        print(f"{'='*80}")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é«˜çº§æ¨¡å‹
    use_advanced_model = getattr(prototype_few_shot_adapt_v7, '_use_advanced_model', False)
    use_channel_attention = getattr(prototype_few_shot_adapt_v7, '_use_channel_attention', True)
    use_residual = getattr(prototype_few_shot_adapt_v7, '_use_residual', True)
    hidden_dim = getattr(prototype_few_shot_adapt_v7, '_hidden_dim', 128)
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Batch Ensemble
    use_batch_ensemble = getattr(prototype_few_shot_adapt_v7, '_use_batch_ensemble', False)
    ensemble_size = getattr(prototype_few_shot_adapt_v7, '_ensemble_size', 4)
    
    if use_advanced_model and HAS_ADVANCED_MODEL:
        # ä½¿ç”¨é«˜çº§æ¨¡å‹ (åŒ…å«é€šé“æ³¨æ„åŠ›ã€æ®‹å·®ã€å¯é€‰BE)
        if verbose:
            print(f"ğŸ¯ ä½¿ç”¨é«˜çº§æ¨¡å‹ (hidden_dim={hidden_dim}, channel_attn={use_channel_attention}, residual={use_residual}, BE={ensemble_size if use_batch_ensemble else 'No'})")
        model = CP_ProtoNet(num_channels=22, feature_dim=19, hidden_dim=hidden_dim).to(device)
        model.feature_extractor = DSTG_Model_V2_Advanced(
            num_channels=22,
            feature_dim=19,
            hidden_dim=hidden_dim,
            num_gcn_layers=4,
            num_heads=8,
            use_channel_attention=use_channel_attention,
            use_residual=use_residual,
            ensemble_size=ensemble_size if use_batch_ensemble else 0
        ).to(device)
    elif use_batch_ensemble and HAS_BATCH_ENSEMBLE:
        if verbose:
            print(f"ğŸ¯ ä½¿ç”¨ Batch Ensemble (ensemble_size={ensemble_size})")
        model = CP_ProtoNet(num_channels=22, feature_dim=19, hidden_dim=128).to(device)
        # æ›¿æ¢ feature_extractor ä¸º Batch Ensemble ç‰ˆæœ¬
        model.feature_extractor = DSTG_Model_V2_BE(
            num_channels=22,
            feature_dim=19,
            hidden_dim=128,
            num_gcn_layers=3,
            num_heads=8,
            ensemble_size=ensemble_size
        ).to(device)
    else:
        model = CP_ProtoNet(num_channels=22, feature_dim=19, hidden_dim=128).to(device)
    proto_manager = SoftPrototypeManager(K=K, feature_dim=128).to(device)
    
    optimizer = optim.Adam(
        list(model.parameters()) + list(proto_manager.parameters()),
        lr=0.001, weight_decay=1e-4
    )
    
    # ç±»åˆ«æƒé‡
    pos_weight = torch.FloatTensor([class_weight]).to(device)
    
    # æ•°æ®å¢å¼º
    use_data_aug = getattr(cp_protonet_loso_v7, '_use_data_augmentation', False)
    if use_data_aug and HAS_DATA_AUGMENTATION:
        mixup_alpha = getattr(cp_protonet_loso_v7, '_mixup_alpha', 0.2)
        noise_std = getattr(cp_protonet_loso_v7, '_noise_std', 0.01)
        augmentor = DataAugmentation(
            use_mixup=True,
            use_noise=True,
            mixup_alpha=mixup_alpha,
            noise_std=noise_std
        )
        if verbose:
            print(f"ğŸ¯ ä½¿ç”¨æ•°æ®å¢å¼º (mixup_alpha={mixup_alpha}, noise_std={noise_std})")
    else:
        augmentor = None
    
    for epoch in range(pretrain_epochs):
        model.train()
        proto_manager.train()
        
        X_batch = torch.FloatTensor(X_train)[:, :418].to(device)
        X_batch = X_batch.view(-1, 22, 19)
        y_batch = torch.LongTensor(y_train).to(device)
        
        # åº”ç”¨æ•°æ®å¢å¼º
        if augmentor is not None:
            X_batch, y_a, y_b, lam = augmentor(X_batch, y_batch, training=True)
        else:
            y_a, y_b, lam = y_batch, y_batch, 1.0
        
        features = model.get_features(X_batch)
        logits, proto_loss, _ = proto_manager(features)
        
        # ä½¿ç”¨åŠ æƒBCE loss (æ”¯æŒ Mixup)
        if augmentor is not None and lam < 1.0:
            ce_loss_a = F.binary_cross_entropy_with_logits(
                logits[:, 1], y_a.float(), pos_weight=pos_weight
            )
            ce_loss_b = F.binary_cross_entropy_with_logits(
                logits[:, 1], y_b.float(), pos_weight=pos_weight
            )
            ce_loss = lam * ce_loss_a + (1 - lam) * ce_loss_b
        else:
            ce_loss = F.binary_cross_entropy_with_logits(
                logits[:, 1], y_batch.float(), pos_weight=pos_weight
            )
        
        loss = ce_loss + 0.1 * proto_loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if verbose and (epoch + 1) % 10 == 0:
            with torch.no_grad():
                probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
                preds = (probs > 0.5).astype(int)
                acc = (preds == y_train).mean()
                sens = ((preds == 1) & (y_train == 1)).sum() / max(y_train.sum(), 1)
                spec = ((preds == 0) & (y_train == 0)).sum() / max((y_train == 0).sum(), 1)
            
            print(f"Epoch {epoch+1}/{pretrain_epochs}: Loss={loss.item():.4f}, "
                  f"Acc={acc:.4f}, Sens={sens:.4f}, Spec={spec:.4f}")
    
    # é˜¶æ®µ2: Few-Shoté€‚åº”
    if verbose:
        print(f"\n{'='*80}")
        print(f"é˜¶æ®µ2: Few-Shoté€‚åº” (epochs={adapt_epochs})")
        print(f"{'='*80}")
    
    if use_advanced_model and HAS_ADVANCED_MODEL:
        # ä½¿ç”¨é«˜çº§æ¨¡å‹
        adapted_model = CP_ProtoNet(num_channels=22, feature_dim=19, hidden_dim=hidden_dim).to(device)
        adapted_model.feature_extractor = DSTG_Model_V2_Advanced(
            num_channels=22,
            feature_dim=19,
            hidden_dim=hidden_dim,
            num_gcn_layers=4,
            num_heads=8,
            use_channel_attention=use_channel_attention,
            use_residual=use_residual,
            ensemble_size=ensemble_size if use_batch_ensemble else 0
        ).to(device)
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        adapted_model.feature_extractor.load_state_dict(model.feature_extractor.state_dict())
        adapted_model.projector.load_state_dict(model.projector.state_dict())
    elif use_batch_ensemble and HAS_BATCH_ENSEMBLE:
        adapted_model = CP_ProtoNet(num_channels=22, feature_dim=19, hidden_dim=128).to(device)
        # æ›¿æ¢ feature_extractor ä¸º Batch Ensemble ç‰ˆæœ¬
        adapted_model.feature_extractor = DSTG_Model_V2_BE(
            num_channels=22,
            feature_dim=19,
            hidden_dim=128,
            num_gcn_layers=3,
            num_heads=8,
            ensemble_size=ensemble_size
        ).to(device)
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        adapted_model.feature_extractor.load_state_dict(model.feature_extractor.state_dict())
        # å¤åˆ¶å…¶ä»–éƒ¨åˆ†
        adapted_model.projector.load_state_dict(model.projector.state_dict())
    else:
        adapted_model = CP_ProtoNet(num_channels=22, feature_dim=19, hidden_dim=128).to(device)
        adapted_model.load_state_dict(model.state_dict())
    
    soft_proto_manager = SoftPrototypeManager(K=K, feature_dim=hidden_dim).to(device)
    soft_proto_manager.load_state_dict(proto_manager.state_dict())
    
    optimizer_adapt = optim.Adam(
        list(adapted_model.parameters()) + list(soft_proto_manager.parameters()),
        lr=0.0005, weight_decay=1e-4
    )
    
    for epoch in range(adapt_epochs):
        adapted_model.train()
        soft_proto_manager.train()
        
        X_adapt_t = torch.FloatTensor(X_adapt)[:, :418].to(device)
        X_adapt_t = X_adapt_t.view(-1, 22, 19)
        y_adapt_t = torch.LongTensor(y_adapt).to(device)
        
        features_adapt = adapted_model.get_features(X_adapt_t)
        logits_adapt, proto_loss_adapt, _ = soft_proto_manager(features_adapt)
        
        # ä½¿ç”¨åŠ æƒBCE loss
        ce_loss_adapt = F.binary_cross_entropy_with_logits(
            logits_adapt[:, 1], y_adapt_t.float(), pos_weight=pos_weight
        )
        
        loss_adapt = ce_loss_adapt + 0.1 * proto_loss_adapt
        
        optimizer_adapt.zero_grad()
        loss_adapt.backward()
        optimizer_adapt.step()
        
        if verbose and (epoch + 1) % 10 == 0:
            with torch.no_grad():
                probs_adapt = torch.softmax(logits_adapt, dim=1)[:, 1].cpu().numpy()
                preds_adapt = (probs_adapt > 0.5).astype(int)
                acc_adapt = (preds_adapt == y_adapt).mean()
                sens_adapt = ((preds_adapt == 1) & (y_adapt == 1)).sum() / max(y_adapt.sum(), 1)
                spec_adapt = ((preds_adapt == 0) & (y_adapt == 0)).sum() / max((y_adapt == 0).sum(), 1)
            
            print(f"Epoch {epoch+1}/{adapt_epochs}: Loss={loss_adapt.item():.4f}, "
                  f"Acc={acc_adapt:.4f}, Sens={sens_adapt:.4f}, Spec={spec_adapt:.4f}")
    
    # é˜¶æ®µ3: å¾®è°ƒé˜ˆå€¼ (0.53*Sens + 0.47*Spec)
    if verbose:
        print(f"\n{'='*80}")
        print(f"é˜¶æ®µ3: å¾®è°ƒé˜ˆå€¼ ({sens_weight}*Sens + {spec_weight}*Spec)")
        print(f"{'='*80}")
    
    adapted_model.eval()
    soft_proto_manager.eval()
    
    with torch.no_grad():
        features_adapt = batch_get_features(adapted_model, X_adapt, device, batch_size=512)
        logits_adapt, _, _ = soft_proto_manager(features_adapt)
        probs_adapt = torch.softmax(logits_adapt, dim=1)[:, 1].cpu().numpy()
    
    # æœç´¢æœ€ä¼˜é˜ˆå€¼
    thresholds = np.linspace(0.1, 0.9, 81)
    best_score = -1
    optimal_thresh = 0.5
    best_metrics = {}
    
    for thresh in thresholds:
        preds = (probs_adapt > thresh).astype(int)
        
        tp = ((preds == 1) & (y_adapt == 1)).sum()
        tn = ((preds == 0) & (y_adapt == 0)).sum()
        fp = ((preds == 1) & (y_adapt == 0)).sum()
        fn = ((preds == 0) & (y_adapt == 1)).sum()
        
        sens = tp / max(tp + fn, 1)
        spec = tn / max(tn + fp, 1)
        
        # å¾®è°ƒæƒé‡: 0.53*Sens + 0.47*Spec
        score = sens_weight * sens + spec_weight * spec
        
        if score > best_score:
            best_score = score
            optimal_thresh = thresh
            best_metrics = {'sensitivity': sens, 'specificity': spec}
    
    if verbose:
        print(f"\nå¾®è°ƒé˜ˆå€¼æœç´¢ ({sens_weight}*Sens + {spec_weight}*Spec):")
        print(f"  æœ€ä¼˜é˜ˆå€¼: {optimal_thresh:.3f}")
        print(f"  åŠ æƒå¾—åˆ†: {best_score:.4f}")
        print(f"  é€‚åº”é›†Sens: {best_metrics['sensitivity']:.4f}")
        print(f"  é€‚åº”é›†Spec: {best_metrics['specificity']:.4f}")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    with torch.no_grad():
        features_eval = batch_get_features(adapted_model, X_eval, device, batch_size=512)
        logits_eval, _, _ = soft_proto_manager(features_eval)
        probs_eval = torch.softmax(logits_eval, dim=1)[:, 1].cpu().numpy()
        preds_eval = (probs_eval > optimal_thresh).astype(int)
    
    metrics = calculate_metrics_with_threshold(
        y_eval, preds_eval, probs_eval, optimal_thresh
    )
    metrics['Strategy'] = f'v11_balanced_{sens_weight}_{spec_weight}'
    
    if verbose:
        print(f"\næµ‹è¯•é›†ç»“æœ:")
        print(f"  AUC: {metrics['AUC']:.4f}")
        print(f"  F1: {metrics['F1']:.4f}")
        print(f"  Sensitivity: {metrics['Sensitivity']:.4f}")
        print(f"  Specificity: {metrics['Specificity']:.4f}")
        print(f"  Precision: {metrics['Precision']:.4f}")
        print(f"  ä½¿ç”¨é˜ˆå€¼: {metrics['Threshold']:.3f} (v11 {sens_weight}:{spec_weight})")
    
    return metrics


if __name__ == "__main__":
    set_seed(42)
    
    # å¿«é€Ÿæµ‹è¯•
    test_patient = 'chb08'  # æµ‹è¯•å›°éš¾æ‚£è€…
    train_patients = ['chb01', 'chb02', 'chb03']
    
    result = cp_protonet_loso_v3(
        test_patient, train_patients,
        pretrain_epochs=30,
        adapt_epochs=20
    )
    
    if result:
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"AUC: {result['AUC']:.4f}")
        print(f"F1: {result['F1']:.4f}")
        print(f"Sens: {result['Sensitivity']:.4f}")
        print(f"Spec: {result['Specificity']:.4f}")
